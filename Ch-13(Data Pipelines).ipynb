{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqex3jKgs68E"
      },
      "source": [
        "## The Data API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4xtBXGixwEb",
        "outputId": "e7266f5c-fc1b-4ed8-8dde-7e71f1e6ab2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<TensorSliceDataset shapes: (), types: tf.int32>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "X = tf.range(10)\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_UW4K35yHm_",
        "outputId": "d1c2d24d-d2b4-45df-8b64-49c4a17d708a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "tf.Tensor(1, shape=(), dtype=int32)\n",
            "tf.Tensor(2, shape=(), dtype=int32)\n",
            "tf.Tensor(3, shape=(), dtype=int32)\n",
            "tf.Tensor(4, shape=(), dtype=int32)\n",
            "tf.Tensor(5, shape=(), dtype=int32)\n",
            "tf.Tensor(6, shape=(), dtype=int32)\n",
            "tf.Tensor(7, shape=(), dtype=int32)\n",
            "tf.Tensor(8, shape=(), dtype=int32)\n",
            "tf.Tensor(9, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for item in dataset:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_rn22doy0NU"
      },
      "source": [
        "### Chaining Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIm-O6xLzCRY",
        "outputId": "5ae57341-59f7-4c9b-9af6-89de68f794d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
            "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
            "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
            "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
            "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "dataset = dataset.repeat(3).batch(7)\n",
        "for item in dataset:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN7XXGxs2oYC",
        "outputId": "b33707ac-e97e-4740-f8ba-24a2f355b77e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
            "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
            "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
            "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
            "tf.Tensor([16 18], shape=(2,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "dataset = dataset.map(lambda x: x*2)\n",
        "for item in dataset:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4PicldsV6WzI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\msi1\\AppData\\Local\\Temp/ipykernel_9424/643490874.py:1: unbatch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.unbatch()`.\n"
          ]
        }
      ],
      "source": [
        "dataset = dataset.apply(tf.data.experimental.unbatch())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "tf.Tensor(2, shape=(), dtype=int32)\n",
            "tf.Tensor(4, shape=(), dtype=int32)\n",
            "tf.Tensor(6, shape=(), dtype=int32)\n",
            "tf.Tensor(8, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for item in dataset.take(5):\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Shuffling the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([0 1 6 5 7 3 9], shape=(7,), dtype=int64)\n",
            "tf.Tensor([8 2 1 0 4 6 4], shape=(7,), dtype=int64)\n",
            "tf.Tensor([7 2 5 9 2 1 3], shape=(7,), dtype=int64)\n",
            "tf.Tensor([4 3 8 7 9 5 0], shape=(7,), dtype=int64)\n",
            "tf.Tensor([8 6], shape=(2,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "tf.random.set_seed(42)\n",
        "dataset = tf.data.Dataset.range(10).repeat(3)\n",
        "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\n",
        "for item in dataset:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target.reshape(-1, 1), \n",
        "                                                    random_state=42)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_mean, X_std = scaler.mean_, scaler.scale_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
        "    housing_dir = os.path.join(\"datasets\", \"housing\")\n",
        "    os.makedirs(housing_dir, exist_ok=True)\n",
        "    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
        "    \n",
        "    filepaths = []\n",
        "    m = len(data)\n",
        "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
        "        part_csv = path_format.format(name_prefix, file_idx)\n",
        "        filepaths.append(part_csv)\n",
        "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
        "            if header is not None:\n",
        "                f.write(header)\n",
        "                f.write(\"\\n\")\n",
        "            for row_idx in row_indices:\n",
        "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
        "                f.write(\"\\n\")\n",
        "    return filepaths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = np.c_[X_train, y_train]\n",
        "test_data = np.c_[X_test, y_test]\n",
        "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
        "header = \",\".join(header_cols)\n",
        "\n",
        "train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header=header, n_parts=20)\n",
        "test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header=header, n_parts=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MedInc</th>\n",
              "      <th>HouseAge</th>\n",
              "      <th>AveRooms</th>\n",
              "      <th>AveBedrms</th>\n",
              "      <th>Population</th>\n",
              "      <th>AveOccup</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "      <th>MedianHouseValue</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.2143</td>\n",
              "      <td>37.0</td>\n",
              "      <td>5.288235</td>\n",
              "      <td>0.973529</td>\n",
              "      <td>860.0</td>\n",
              "      <td>2.529412</td>\n",
              "      <td>33.81</td>\n",
              "      <td>-118.12</td>\n",
              "      <td>2.285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.3468</td>\n",
              "      <td>42.0</td>\n",
              "      <td>6.364322</td>\n",
              "      <td>1.087940</td>\n",
              "      <td>957.0</td>\n",
              "      <td>2.404523</td>\n",
              "      <td>37.16</td>\n",
              "      <td>-121.98</td>\n",
              "      <td>2.799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.9191</td>\n",
              "      <td>36.0</td>\n",
              "      <td>6.110063</td>\n",
              "      <td>1.059748</td>\n",
              "      <td>711.0</td>\n",
              "      <td>2.235849</td>\n",
              "      <td>38.45</td>\n",
              "      <td>-122.69</td>\n",
              "      <td>1.830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6.3703</td>\n",
              "      <td>32.0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.990196</td>\n",
              "      <td>1159.0</td>\n",
              "      <td>2.272549</td>\n",
              "      <td>34.16</td>\n",
              "      <td>-118.41</td>\n",
              "      <td>4.658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.3684</td>\n",
              "      <td>17.0</td>\n",
              "      <td>4.795858</td>\n",
              "      <td>1.035503</td>\n",
              "      <td>706.0</td>\n",
              "      <td>2.088757</td>\n",
              "      <td>38.57</td>\n",
              "      <td>-121.33</td>\n",
              "      <td>1.500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
              "0  4.2143      37.0  5.288235   0.973529       860.0  2.529412     33.81   \n",
              "1  5.3468      42.0  6.364322   1.087940       957.0  2.404523     37.16   \n",
              "2  3.9191      36.0  6.110063   1.059748       711.0  2.235849     38.45   \n",
              "3  6.3703      32.0  6.000000   0.990196      1159.0  2.272549     34.16   \n",
              "4  2.3684      17.0  4.795858   1.035503       706.0  2.088757     38.57   \n",
              "\n",
              "   Longitude  MedianHouseValue  \n",
              "0    -118.12             2.285  \n",
              "1    -121.98             2.799  \n",
              "2    -122.69             1.830  \n",
              "3    -118.41             4.658  \n",
              "4    -121.33             1.500  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.read_csv(train_filepaths[0]).head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n",
            "4.2143,37.0,5.288235294117647,0.9735294117647059,860.0,2.5294117647058822,33.81,-118.12,2.285\n",
            "5.3468,42.0,6.364321608040201,1.0879396984924623,957.0,2.4045226130653266,37.16,-121.98,2.799\n",
            "3.9191,36.0,6.110062893081761,1.059748427672956,711.0,2.2358490566037736,38.45,-122.69,1.83\n",
            "6.3703,32.0,6.0,0.9901960784313726,1159.0,2.272549019607843,34.16,-118.41,4.658\n"
          ]
        }
      ],
      "source": [
        "with open(train_filepaths[0]) as f:\n",
        "    for i in range(5):\n",
        "        print(f.readline(), end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building an Input Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'datasets\\\\housing\\\\my_train_05.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets\\\\housing\\\\my_train_16.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets\\\\housing\\\\my_train_01.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets\\\\housing\\\\my_train_17.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets\\\\housing\\\\my_train_00.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets\\\\housing\\\\my_train_14.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets\\\\housing\\\\my_train_10.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets\\\\housing\\\\my_train_02.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets\\\\housing\\\\my_train_12.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets\\\\housing\\\\my_train_19.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets\\\\housing\\\\my_train_07.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets\\\\housing\\\\my_train_09.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets\\\\housing\\\\my_train_13.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets\\\\housing\\\\my_train_15.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets\\\\housing\\\\my_train_11.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets\\\\housing\\\\my_train_18.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets\\\\housing\\\\my_train_04.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets\\\\housing\\\\my_train_06.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets\\\\housing\\\\my_train_03.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets\\\\housing\\\\my_train_08.csv', shape=(), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "for filepath in filepath_dataset:\n",
        "    print(filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_reader = 5\n",
        "dataset = filepath_dataset.interleave(\n",
        "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1), \n",
        "    cycle_length=n_reader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'3.3125,11.0,5.361736334405145,1.0578778135048232,1963.0,3.1559485530546625,38.69,-121.46,0.968'\n",
            "b'2.4524,41.0,5.340116279069767,1.188953488372093,1430.0,4.156976744186046,34.13,-117.32,0.704'\n",
            "b'5.8735,35.0,5.811638591117918,1.0566615620214395,1521.0,2.329249617151608,34.11,-118.63,4.481'\n",
            "b'3.8788,20.0,5.140068886337543,1.060849598163031,2656.0,3.049368541905855,37.13,-121.66,2.269'\n",
            "b'4.5893,39.0,5.711688311688311,1.0077922077922077,1025.0,2.6623376623376624,37.95,-122.07,1.9'\n"
          ]
        }
      ],
      "source": [
        "for line in dataset.take(5):\n",
        "    print(line.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Processing the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_inputs = 8\n",
        "\n",
        "def preprocess(line):\n",
        "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
        "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
        "    x = tf.stack(fields[:-1])\n",
        "    y = tf.stack(fields[-1:])\n",
        "    return (x - X_mean)/X_std, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
              " array([ 0.17648865,  0.66640687, -0.06085434, -0.2811182 , -0.49654418,\n",
              "        -0.04828325, -0.86074144,  0.73099613], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.285], dtype=float32)>)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preprocess(b'4.2143,37.0,5.288235294117647,0.9735294117647059,860.0,2.5294117647058822,33.81,-118.12,2.285')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def csv_reader_dataset(filepaths, repeat=1, n_readers=5, \n",
        "                       n_read_threads=None, shuffle_buffer_size=10000, \n",
        "                       n_parse_threads=5, batch_size=32):\n",
        "    dataset = tf.data.Dataset.list_files(filepaths)\n",
        "    dataset = dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1), \n",
        "                                 cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
        "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
        "    dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat)\n",
        "    return dataset.batch(batch_size).prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([   4.2143    ,   37.        ,    5.28823529,    0.97352941,\n",
              "        860.        ,    2.52941176,   33.81      , -118.12      ])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using Dataset with tf.keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_set = csv_reader_dataset(train_filepaths)\n",
        "test_set = csv_reader_dataset(test_filepaths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "484/484 [==============================] - 2s 2ms/step - loss: 0.5514 - mae: 0.5358 - val_loss: 0.5612 - val_mae: 0.5202\n",
            "Epoch 2/100\n",
            "484/484 [==============================] - 1s 1ms/step - loss: 0.4150 - mae: 0.4621 - val_loss: 0.3551 - val_mae: 0.4313\n",
            "Epoch 3/100\n",
            "484/484 [==============================] - 1s 1ms/step - loss: 0.3963 - mae: 0.4492 - val_loss: 0.4187 - val_mae: 0.4794\n",
            "Epoch 4/100\n",
            "484/484 [==============================] - 1s 1ms/step - loss: 0.3832 - mae: 0.4426 - val_loss: 0.5200 - val_mae: 0.4968\n",
            "Epoch 5/100\n",
            "484/484 [==============================] - 1s 1ms/step - loss: 0.3792 - mae: 0.4359 - val_loss: 0.4806 - val_mae: 0.5197\n",
            "Epoch 6/100\n",
            "484/484 [==============================] - 1s 2ms/step - loss: 0.3643 - mae: 0.4304 - val_loss: 0.3219 - val_mae: 0.4059\n",
            "Epoch 7/100\n",
            "484/484 [==============================] - 1s 1ms/step - loss: 0.3563 - mae: 0.4240 - val_loss: 0.4373 - val_mae: 0.4445\n",
            "Epoch 8/100\n",
            "484/484 [==============================] - 1s 1ms/step - loss: 0.3542 - mae: 0.4233 - val_loss: 0.4294 - val_mae: 0.4442\n",
            "Epoch 9/100\n",
            "484/484 [==============================] - 1s 1ms/step - loss: 0.3489 - mae: 0.4207 - val_loss: 0.4071 - val_mae: 0.4907\n",
            "Epoch 10/100\n",
            "484/484 [==============================] - 1s 1ms/step - loss: 0.3555 - mae: 0.4265 - val_loss: 0.3989 - val_mae: 0.4681\n",
            "Epoch 11/100\n",
            "484/484 [==============================] - 1s 1ms/step - loss: 0.3401 - mae: 0.4151 - val_loss: 0.3602 - val_mae: 0.3993\n",
            "Epoch 12/100\n",
            "484/484 [==============================] - 1s 1ms/step - loss: 0.3499 - mae: 0.4227 - val_loss: 0.3358 - val_mae: 0.3846\n",
            "Epoch 13/100\n",
            "484/484 [==============================] - 1s 1ms/step - loss: 0.3502 - mae: 0.4231 - val_loss: 0.4278 - val_mae: 0.5148\n",
            "Epoch 14/100\n",
            "484/484 [==============================] - 1s 1ms/step - loss: 0.3369 - mae: 0.4141 - val_loss: 0.3552 - val_mae: 0.4362\n",
            "Epoch 15/100\n",
            "484/484 [==============================] - 1s 2ms/step - loss: 0.3415 - mae: 0.4167 - val_loss: 0.4747 - val_mae: 0.4915\n",
            "Epoch 16/100\n",
            "484/484 [==============================] - 1s 1ms/step - loss: 0.3362 - mae: 0.4117 - val_loss: 0.4302 - val_mae: 0.4429\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x24bab27f2b0>"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import optimizers, callbacks\n",
        "model = keras.Sequential([keras.layers.Dense(100, activation=\"relu\"), \n",
        "                          keras.layers.BatchNormalization(), \n",
        "                          keras.layers.Dense(200, activation=\"relu\"), \n",
        "                          keras.layers.BatchNormalization(), \n",
        "                          keras.layers.Dense(10, activation=\"relu\"), \n",
        "                          keras.layers.BatchNormalization(), \n",
        "                          keras.layers.Dense(1)])\n",
        "model.compile(loss=\"mse\", optimizer=optimizers.SGD(learning_rate=1e-2, nesterov=True, \n",
        "                                                   momentum=0.93), metrics=[\"mae\"])\n",
        "model.fit(train_set, epochs=100, \n",
        "          validation_data=test_set, \n",
        "          callbacks=[callbacks.EarlyStopping(patience=10, \n",
        "                                             restore_best_weights=True, \n",
        "                                             monitor=\"val_loss\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "162/162 [==============================] - 0s 1ms/step - loss: 0.3219 - mae: 0.4059\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.3218575417995453, 0.4059430658817291]"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[2.9361138 ],\n",
              "       [0.9133055 ],\n",
              "       [2.434945  ],\n",
              "       [2.3590877 ],\n",
              "       [1.2241691 ],\n",
              "       [2.530606  ],\n",
              "       [4.6654253 ],\n",
              "       [2.7495708 ],\n",
              "       [0.93317914],\n",
              "       [2.0079727 ],\n",
              "       [3.4963975 ],\n",
              "       [2.6451511 ],\n",
              "       [2.0391731 ],\n",
              "       [1.194759  ],\n",
              "       [1.3651963 ],\n",
              "       [5.110795  ],\n",
              "       [3.0155265 ],\n",
              "       [1.4997151 ],\n",
              "       [2.5836954 ],\n",
              "       [1.0702391 ],\n",
              "       [2.1778834 ],\n",
              "       [1.0713279 ],\n",
              "       [0.9217081 ],\n",
              "       [2.8020098 ],\n",
              "       [1.7460865 ],\n",
              "       [2.945178  ],\n",
              "       [1.7114737 ],\n",
              "       [2.0891716 ],\n",
              "       [2.305393  ],\n",
              "       [2.0068982 ],\n",
              "       [1.7338572 ],\n",
              "       [2.7331486 ],\n",
              "       [3.7790947 ],\n",
              "       [0.9678383 ],\n",
              "       [2.425568  ],\n",
              "       [2.6264007 ],\n",
              "       [1.9635203 ],\n",
              "       [1.2492391 ],\n",
              "       [1.0608734 ],\n",
              "       [2.624142  ],\n",
              "       [2.179114  ],\n",
              "       [1.9583938 ],\n",
              "       [0.8439145 ],\n",
              "       [1.301668  ],\n",
              "       [1.7930489 ],\n",
              "       [1.8987231 ],\n",
              "       [1.1935368 ],\n",
              "       [2.2147698 ],\n",
              "       [3.7301488 ],\n",
              "       [3.182203  ],\n",
              "       [2.209907  ],\n",
              "       [2.4681718 ],\n",
              "       [2.8485055 ],\n",
              "       [2.186312  ],\n",
              "       [1.9342339 ],\n",
              "       [1.6700764 ],\n",
              "       [1.2814641 ],\n",
              "       [2.0246856 ],\n",
              "       [2.1582336 ],\n",
              "       [2.3998184 ],\n",
              "       [2.0721788 ],\n",
              "       [2.1509278 ],\n",
              "       [2.8144794 ],\n",
              "       [2.6759672 ],\n",
              "       [2.6319306 ],\n",
              "       [2.6405406 ],\n",
              "       [1.7776796 ],\n",
              "       [2.1983848 ],\n",
              "       [2.9530194 ],\n",
              "       [5.768956  ],\n",
              "       [2.238689  ],\n",
              "       [1.4894311 ],\n",
              "       [1.553855  ],\n",
              "       [4.4035435 ],\n",
              "       [2.30749   ],\n",
              "       [1.8487935 ],\n",
              "       [1.7697408 ],\n",
              "       [1.4272227 ],\n",
              "       [1.5867543 ],\n",
              "       [1.890112  ],\n",
              "       [3.5849648 ],\n",
              "       [2.646664  ],\n",
              "       [2.9345837 ],\n",
              "       [1.131978  ],\n",
              "       [1.3661853 ],\n",
              "       [1.4781072 ],\n",
              "       [3.4416242 ],\n",
              "       [0.8668957 ],\n",
              "       [2.9001527 ],\n",
              "       [2.401717  ],\n",
              "       [1.9755334 ],\n",
              "       [1.8014824 ],\n",
              "       [1.6597252 ],\n",
              "       [1.5336657 ],\n",
              "       [3.0155883 ],\n",
              "       [1.1764789 ]], dtype=float32)"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_set = test_set.take(3).map(lambda X, y: X)\n",
        "model.predict(new_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom function to train model\n",
        "\n",
        "@tf.function\n",
        "def train(model, optimizers, loss_fn, n_epochs, **kwargs):\n",
        "    train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs)\n",
        "    for X_batch, y_batch in train_set:\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model(X_batch)\n",
        "            main_loss = loss_fn(y_batch, y_pred)\n",
        "            loss = tf.add_n([main_loss] + model.losses)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The TFRecord Format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating a TFRecord dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
            "tf.Tensor(b'And, This is the second Record.', shape=(), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
        "    f.write(b\"This is the first record\")\n",
        "    f.write(b\"And, This is the second Record.\")\n",
        "filepaths = [\"my_data.tfrecord\"]\n",
        "dataset = tf.data.TFRecordDataset(filepaths)\n",
        "for item in dataset:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compressed TFRecord Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'This is First One', shape=(), dtype=string)\n",
            "tf.Tensor(b'And the Another One', shape=(), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
        "with tf.io.TFRecordWriter(\"my_compressed_data.tfrecord\", options) as f:\n",
        "    f.write(\"This is First One\")\n",
        "    f.write(\"And the Another One\")\n",
        "dataset = tf.data.TFRecordDataset([\"my_compressed_data.tfrecord\"], \n",
        "                                  compression_type=\"GZIP\")\n",
        "for item in dataset:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Protocol Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.train import BytesList, FloatList, Int64List\n",
        "from tensorflow.train import Feature, Features, Example\n",
        "\n",
        "person_example = Example(\n",
        "    features=Features(\n",
        "        feature={\n",
        "            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])), \n",
        "            \"id\": Feature(int64_list=Int64List(value=[123])), \n",
        "            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\", \n",
        "                                                          b\"c@d.com\"]))\n",
        "        }))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "with tf.io.TFRecordWriter(\"my_contact.tfrecord\") as f:\n",
        "    f.write(person_example.SerializeToString())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading and Parsing Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_description = {\n",
        "    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"), \n",
        "    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0), \n",
        "    \"emails\": tf.io.VarLenFeature(tf.string),\n",
        "}\n",
        "\n",
        "for serialized_example in tf.data.TFRecordDataset([\"my_contact.tfrecord\"]):\n",
        "    parsed_example = tf.io.parse_single_example(serialized_example, \n",
        "                                               feature_description)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'a@b.com', b'c@d.com'], dtype=object)>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.sparse.to_dense(parsed_example[\"emails\"], default_value=b\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'a@b.com', b'c@d.com'], dtype=object)>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parsed_example[\"emails\"].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = tf.data.TFRecordDataset([\"my_contact.tfrecord\"]).batch(10)\n",
        "for serialized_example in dataset:\n",
        "    parsed_examples = tf.io.parse_example(serialized_example, \n",
        "                                          feature_description)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Handling Lists of List Using SequenceExample ProtoBuf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "FeatureList = tf.train.FeatureList\n",
        "FeatureLists = tf.train.FeatureLists\n",
        "SequenceExample = tf.train.SequenceExample\n",
        "\n",
        "context = Features(feature={\n",
        "    \"author_id\": Feature(int64_list=Int64List(value=[123])), \n",
        "    \"title\": Feature(bytes_list=BytesList(value=[b\"A\", b\"desert\", b\"place\", b\".\"])), \n",
        "    \"pub_date\": Feature(int64_list=Int64List(value=[1623, 12, 25]))\n",
        "})\n",
        "\n",
        "content = [[\"When\", \"shall\", \"we\", \"three\", \"meet\", \"again\", \"?\"], \n",
        "           [\"In\", \"thunder\", \",\", \"lightning\", \",\", \"or\", \"in\", \"rain\", \"?\"]]\n",
        "comments = [[\"When\", \"the\", \"hurlyburly\", \"'s\", \"done\", \".\"], \n",
        "            [\"When\", \"the\", \"battle\", \"'s\", \"lost\", \"and\", \"won\", \".\"]]\n",
        "\n",
        "def words_to_feature(words):\n",
        "    return Feature(bytes_list=BytesList(value=[word.encode(\"utf-8\") for word in words]))\n",
        "\n",
        "content_features = [words_to_feature(sentence) for sentence in content]\n",
        "comment_features = [words_to_feature(comment) for comment in comments]\n",
        "\n",
        "sequence_example = SequenceExample(\n",
        "    context=context, \n",
        "    feature_lists=FeatureLists(feature_list={\n",
        "        \"content\": FeatureList(feature=content_features), \n",
        "        \"comments\": FeatureList(feature=comment_features)\n",
        "    })\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "context_feature_descriptions = {\n",
        "    \"author_id\": tf.io.FixedLenFeature([], tf.int64, default_value=0), \n",
        "    \"title\": tf.io.VarLenFeature(tf.string), \n",
        "    \"pub_data\": tf.io.FixedLenFeature([3], tf.int64, default_value=[0, 0, 0]), \n",
        "}\n",
        "\n",
        "sequence_feature_descriptions = {\n",
        "    \"content\": tf.io.VarLenFeature(tf.string), \n",
        "    \"comments\": tf.io.VarLenFeature(tf.string), \n",
        "}\n",
        "\n",
        "serialized_sequence_example = sequence_example.SerializeToString()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "parsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example(\n",
        "    serialized_sequence_example, context_feature_descriptions, \n",
        "    sequence_feature_descriptions)\n",
        "parsed_content = tf.RaggedTensor.from_sparse(parsed_feature_lists[\"content\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'title': <tensorflow.python.framework.sparse_tensor.SparseTensor at 0x274a6b47af0>,\n",
              " 'author_id': <tf.Tensor: shape=(), dtype=int64, numpy=123>,\n",
              " 'pub_data': <tf.Tensor: shape=(3,), dtype=int64, numpy=array([0, 0, 0], dtype=int64)>}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parsed_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4,), dtype=string, numpy=array([b'A', b'desert', b'place', b'.'], dtype=object)>"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parsed_context[\"title\"].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'comments': <tensorflow.python.framework.sparse_tensor.SparseTensor at 0x274ade08160>,\n",
              " 'content': <tensorflow.python.framework.sparse_tensor.SparseTensor at 0x274ae02c9d0>}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parsed_feature_lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'When', b'shall', b'we', b'three', b'meet', b'again', b'?'], [b'In', b'thunder', b',', b'lightning', b',', b'or', b'in', b'rain', b'?']]>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parsed_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing the Input Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Ch-13(Data Pipelines).ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "81727fb6587a0e5418c3cf386f53a5fb924b641d49e181fa8e28403b9a38d60e"
    },
    "kernelspec": {
      "display_name": "Python 3.8.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
