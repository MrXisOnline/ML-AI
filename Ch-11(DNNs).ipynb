{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing/Exploding Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x1f812ceb670>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation=\"relu\", \n",
    "                    kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x1f812dadb50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "he_avg_init = keras.initializers.VarianceScaling(scale=2, \n",
    "                                                mode=\"fan_avg\", \n",
    "                                                distribution=\"uniform\")\n",
    "keras.layers.Dense(10, activation=\"relu\", \n",
    "                    kernel_initializer=he_avg_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x1f812dadd30>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using leaky relu\n",
    "leaky_relu = keras.layers.LeakyReLU(alpha=0.2)\n",
    "keras.layers.Dense(10, activation=leaky_relu, \n",
    "                    kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x1f812dad970>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation=\"selu\", \n",
    "                    kernel_initializer=\"lecun_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]), \n",
    "    keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(300, activation=\"elu\", \n",
    "                        kernel_initializer=\"he_normal\"), \n",
    "    keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(100, activation=\"elu\", \n",
    "                        kernel_initializer=\"he_normal\"), \n",
    "    keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 784)              3136      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization_3/gamma:0', True),\n",
       " ('batch_normalization_3/beta:0', True),\n",
       " ('batch_normalization_3/moving_mean:0', False),\n",
       " ('batch_normalization_3/moving_variance:0', False)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SG704\\AppData\\Local\\Temp/ipykernel_12816/3873162892.py:1: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  model.layers[1].updates\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]), \n",
    "    keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(300, \n",
    "        kernel_initializer=\"he_normal\", \n",
    "        use_bias=False), \n",
    "    keras.layers.BatchNormalization(), \n",
    "    keras.layers.Activation(\"elu\"), \n",
    "    keras.layers.Dense(100, \n",
    "        kernel_initializer=\"he_normal\", \n",
    "        use_bias=False), \n",
    "    keras.layers.BatchNormalization(), \n",
    "    keras.layers.Activation(\"elu\"), \n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "def get_data_without_5_6(data, labels):\n",
    "    new_data, new_labels = [], []\n",
    "    for i, label in enumerate(labels):\n",
    "        if (label != 5) & (label != 6):\n",
    "            if label > 5:\n",
    "                new_labels.append(labels[i]-2)\n",
    "            else:\n",
    "                new_labels.append(labels[i])\n",
    "            new_data.append(data[i])\n",
    "    return np.array(new_data), np.array(new_labels)\n",
    "        \n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_A, y_train_A = get_data_without_5_6(X_train, y_train)\n",
    "X_test_A, y_test_A = get_data_without_5_6(X_test, y_test)\n",
    "X_train_A, X_val_A = X_train_A[:30000], X_train_A[30000:]\n",
    "y_train_A, y_val_A = y_train_A[:30000], y_train_A[30000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "938/938 [==============================] - 7s 5ms/step - loss: 0.3977 - accuracy: 0.8659 - val_loss: 0.2692 - val_accuracy: 0.9088\n",
      "Epoch 2/100\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.2771 - accuracy: 0.9043 - val_loss: 0.2425 - val_accuracy: 0.9173\n",
      "Epoch 3/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.2402 - accuracy: 0.9177 - val_loss: 0.2338 - val_accuracy: 0.9207\n",
      "Epoch 4/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.2150 - accuracy: 0.9246 - val_loss: 0.2207 - val_accuracy: 0.9251\n",
      "Epoch 5/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.2044 - accuracy: 0.9281 - val_loss: 0.2209 - val_accuracy: 0.9233\n",
      "Epoch 6/100\n",
      "938/938 [==============================] - 3s 4ms/step - loss: 0.1892 - accuracy: 0.9326 - val_loss: 0.2141 - val_accuracy: 0.9277\n",
      "Epoch 7/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.1761 - accuracy: 0.9371 - val_loss: 0.2168 - val_accuracy: 0.9251\n",
      "Epoch 8/100\n",
      "938/938 [==============================] - 3s 4ms/step - loss: 0.1654 - accuracy: 0.9397 - val_loss: 0.2088 - val_accuracy: 0.9282\n",
      "Epoch 9/100\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.1579 - accuracy: 0.9439 - val_loss: 0.2177 - val_accuracy: 0.92680s - loss: 0\n",
      "Epoch 10/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.1511 - accuracy: 0.9464 - val_loss: 0.2061 - val_accuracy: 0.9316\n",
      "Epoch 11/100\n",
      "938/938 [==============================] - 3s 4ms/step - loss: 0.1448 - accuracy: 0.9490 - val_loss: 0.2025 - val_accuracy: 0.9320\n",
      "Epoch 12/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.1380 - accuracy: 0.9515 - val_loss: 0.2066 - val_accuracy: 0.9319\n",
      "Epoch 13/100\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.1313 - accuracy: 0.9533 - val_loss: 0.2161 - val_accuracy: 0.9277\n",
      "Epoch 14/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.1260 - accuracy: 0.9557 - val_loss: 0.2042 - val_accuracy: 0.9331\n",
      "Epoch 15/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.1211 - accuracy: 0.9565 - val_loss: 0.2101 - val_accuracy: 0.9299\n",
      "Epoch 16/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.1123 - accuracy: 0.9605 - val_loss: 0.2117 - val_accuracy: 0.9294\n",
      "Epoch 17/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.1090 - accuracy: 0.9610 - val_loss: 0.2098 - val_accuracy: 0.9306\n",
      "Epoch 18/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.1025 - accuracy: 0.9633 - val_loss: 0.2136 - val_accuracy: 0.9303\n",
      "Epoch 19/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0990 - accuracy: 0.9650 - val_loss: 0.2125 - val_accuracy: 0.9317\n",
      "Epoch 20/100\n",
      "938/938 [==============================] - 3s 4ms/step - loss: 0.0943 - accuracy: 0.9668 - val_loss: 0.2237 - val_accuracy: 0.9288\n",
      "Epoch 21/100\n",
      "938/938 [==============================] - 3s 4ms/step - loss: 0.0920 - accuracy: 0.9662 - val_loss: 0.2159 - val_accuracy: 0.9317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24762e9e580>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_A = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)), \n",
    "    keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(300, activation=\"relu\"), \n",
    "    keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(100, activation=\"relu\"), \n",
    "    keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(8, activation=\"softmax\")\n",
    "])\n",
    "model_A.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "                optimizer=\"sgd\", \n",
    "                metrics=\"accuracy\")\n",
    "model_A.fit(X_train_A, y_train_A, epochs=100, \n",
    "            validation_data=(X_val_A, y_val_A), \n",
    "            callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
    "                                                     patience=10, \n",
    "                                                     restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2272 - accuracy: 0.9243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.22721777856349945, 0.9242500066757202]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_A.evaluate(X_test_A, y_test_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_withonly_5_6(data, labels):\n",
    "    new_data, new_labels = [], []\n",
    "    for i, label in enumerate(labels):\n",
    "        if label == 5:\n",
    "            new_data.append(data[i])\n",
    "            new_labels.append(1)\n",
    "        elif label == 6:\n",
    "            new_data.append(data[i])\n",
    "            new_labels.append(0)\n",
    "    return np.array(new_data), np.array(new_labels)\n",
    "\n",
    "X_train_B, y_train_B = get_data_withonly_5_6(X_train, y_train)\n",
    "X_test_B, y_test_B = get_data_withonly_5_6(X_test, y_test)\n",
    "X_train_B, X_val_B = X_train_B[:10000], X_train_B[10000:]\n",
    "y_train_B, y_val_B = y_train_B[:10000], y_train_B[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.1196 - accuracy: 0.9791 - val_loss: 0.0525 - val_accuracy: 0.9920\n",
      "Epoch 2/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0477 - accuracy: 0.9923 - val_loss: 0.0312 - val_accuracy: 0.9925\n",
      "Epoch 3/100\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0296 - accuracy: 0.9956 - val_loss: 0.0316 - val_accuracy: 0.9930\n",
      "Epoch 4/100\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0199 - accuracy: 0.9970 - val_loss: 0.0238 - val_accuracy: 0.9950\n",
      "Epoch 5/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0184 - accuracy: 0.9965 - val_loss: 0.0205 - val_accuracy: 0.9950\n",
      "Epoch 6/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0148 - accuracy: 0.9980 - val_loss: 0.0240 - val_accuracy: 0.9950\n",
      "Epoch 7/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0123 - accuracy: 0.9979 - val_loss: 0.0163 - val_accuracy: 0.9955\n",
      "Epoch 8/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0128 - accuracy: 0.9969 - val_loss: 0.0149 - val_accuracy: 0.9960\n",
      "Epoch 9/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0116 - accuracy: 0.9975 - val_loss: 0.0154 - val_accuracy: 0.9955\n",
      "Epoch 10/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0093 - accuracy: 0.9985 - val_loss: 0.0118 - val_accuracy: 0.9965\n",
      "Epoch 11/100\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0092 - accuracy: 0.9984 - val_loss: 0.0134 - val_accuracy: 0.9960\n",
      "Epoch 12/100\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0089 - accuracy: 0.9979 - val_loss: 0.0122 - val_accuracy: 0.9970\n",
      "Epoch 13/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0079 - accuracy: 0.9984 - val_loss: 0.0112 - val_accuracy: 0.9965\n",
      "Epoch 14/100\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0062 - accuracy: 0.9993 - val_loss: 0.0112 - val_accuracy: 0.9970\n",
      "Epoch 15/100\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0077 - accuracy: 0.9982 - val_loss: 0.0133 - val_accuracy: 0.9965\n",
      "Epoch 16/100\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0048 - accuracy: 0.9994 - val_loss: 0.0124 - val_accuracy: 0.9965\n",
      "Epoch 17/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0065 - accuracy: 0.9988 - val_loss: 0.0109 - val_accuracy: 0.9975\n",
      "Epoch 18/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0071 - accuracy: 0.9987 - val_loss: 0.0122 - val_accuracy: 0.9970\n",
      "Epoch 19/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0050 - accuracy: 0.9994 - val_loss: 0.0122 - val_accuracy: 0.9975\n",
      "Epoch 20/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0147 - val_accuracy: 0.9970\n",
      "Epoch 21/100\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0056 - accuracy: 0.9990 - val_loss: 0.0106 - val_accuracy: 0.9975\n",
      "Epoch 22/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0046 - accuracy: 0.9993 - val_loss: 0.0124 - val_accuracy: 0.9975\n",
      "Epoch 23/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0054 - accuracy: 0.9989 - val_loss: 0.0121 - val_accuracy: 0.9975\n",
      "Epoch 24/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0058 - accuracy: 0.9990 - val_loss: 0.0117 - val_accuracy: 0.9975\n",
      "Epoch 25/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0035 - accuracy: 0.9994 - val_loss: 0.0123 - val_accuracy: 0.9975\n",
      "Epoch 26/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0046 - accuracy: 0.9993 - val_loss: 0.0160 - val_accuracy: 0.9965\n",
      "Epoch 27/100\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0036 - accuracy: 0.9995 - val_loss: 0.0131 - val_accuracy: 0.9970\n",
      "Epoch 28/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0026 - accuracy: 0.9998 - val_loss: 0.0108 - val_accuracy: 0.9970\n",
      "Epoch 29/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0044 - accuracy: 0.9993 - val_loss: 0.0113 - val_accuracy: 0.9970\n",
      "Epoch 30/100\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0028 - accuracy: 0.9996 - val_loss: 0.0111 - val_accuracy: 0.9970\n",
      "Epoch 31/100\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0025 - accuracy: 0.9998 - val_loss: 0.0101 - val_accuracy: 0.9970\n",
      "Epoch 32/100\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0033 - accuracy: 0.9995 - val_loss: 0.0108 - val_accuracy: 0.9970\n",
      "Epoch 33/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0026 - accuracy: 0.9998 - val_loss: 0.0128 - val_accuracy: 0.9970\n",
      "Epoch 34/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0030 - accuracy: 0.9996 - val_loss: 0.0118 - val_accuracy: 0.9975\n",
      "Epoch 35/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0026 - accuracy: 0.9997 - val_loss: 0.0116 - val_accuracy: 0.9970\n",
      "Epoch 36/100\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0111 - val_accuracy: 0.9975\n",
      "Epoch 37/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9989 - val_loss: 0.0182 - val_accuracy: 0.9970\n",
      "Epoch 38/100\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0021 - accuracy: 0.9998 - val_loss: 0.0137 - val_accuracy: 0.9970\n",
      "Epoch 39/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0135 - val_accuracy: 0.9970\n",
      "Epoch 40/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0109 - val_accuracy: 0.9975\n",
      "Epoch 41/100\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.0107 - val_accuracy: 0.9975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2476345fa90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)), \n",
    "    keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(300, activation=\"relu\"), \n",
    "    keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(100, activation=\"relu\"), \n",
    "    keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(10, activation=\"relu\"), \n",
    "    keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model_B.compile(loss=\"binary_crossentropy\", \n",
    "                optimizer=\"sgd\", \n",
    "                metrics=\"accuracy\")\n",
    "model_B.fit(X_train_B, y_train_B, epochs=100, \n",
    "            validation_data=(X_val_B, y_val_B), \n",
    "            callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
    "                                                     patience=10, \n",
    "                                                     restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0076 - accuracy: 0.9985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0075619034469127655, 0.9984999895095825]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2272 - accuracy: 0.9243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.22721777856349945, 0.9242500066757202]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_A.evaluate(X_test_A, y_test_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B_on_A = keras.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", \n",
    "                     optimizer=\"sgd\", \n",
    "                     metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1328 - accuracy: 0.9611 - val_loss: 0.0554 - val_accuracy: 0.9880\n",
      "Epoch 2/4\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0345 - accuracy: 0.9950 - val_loss: 0.0379 - val_accuracy: 0.9905\n",
      "Epoch 3/4\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0238 - accuracy: 0.9962 - val_loss: 0.0313 - val_accuracy: 0.9910\n",
      "Epoch 4/4\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0191 - accuracy: 0.9967 - val_loss: 0.0280 - val_accuracy: 0.9910\n"
     ]
    }
   ],
   "source": [
    "history_b_on_A = model_B_on_A.fit(X_train_B, y_train_B, epochs=4, \n",
    "                                  validation_data=(X_val_B, y_val_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "313/313 [==============================] - 2s 4ms/step - loss: 0.0403 - accuracy: 0.9886 - val_loss: 0.0410 - val_accuracy: 0.9905\n",
      "Epoch 2/16\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0333 - accuracy: 0.9916 - val_loss: 0.0398 - val_accuracy: 0.9915\n",
      "Epoch 3/16\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0332 - accuracy: 0.9919 - val_loss: 0.0361 - val_accuracy: 0.9915\n",
      "Epoch 4/16\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0327 - accuracy: 0.9916 - val_loss: 0.0353 - val_accuracy: 0.9910\n",
      "Epoch 5/16\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.0334 - accuracy: 0.9917 - val_loss: 0.0327 - val_accuracy: 0.9910\n",
      "Epoch 6/16\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0312 - accuracy: 0.9919 - val_loss: 0.0321 - val_accuracy: 0.9915\n",
      "Epoch 7/16\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.0268 - accuracy: 0.9946 - val_loss: 0.0312 - val_accuracy: 0.9915\n",
      "Epoch 8/16\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0267 - accuracy: 0.9938 - val_loss: 0.0317 - val_accuracy: 0.9915\n",
      "Epoch 9/16\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0259 - accuracy: 0.9935 - val_loss: 0.0303 - val_accuracy: 0.9910\n",
      "Epoch 10/16\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0283 - accuracy: 0.9927 - val_loss: 0.0309 - val_accuracy: 0.9910\n",
      "Epoch 11/16\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0262 - accuracy: 0.9933 - val_loss: 0.0286 - val_accuracy: 0.9915\n",
      "Epoch 12/16\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0237 - accuracy: 0.9952 - val_loss: 0.0288 - val_accuracy: 0.9915\n",
      "Epoch 13/16\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0244 - accuracy: 0.9943 - val_loss: 0.0276 - val_accuracy: 0.9915\n",
      "Epoch 14/16\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0243 - accuracy: 0.9940 - val_loss: 0.0287 - val_accuracy: 0.9915\n",
      "Epoch 15/16\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 0.0241 - accuracy: 0.9946 - val_loss: 0.0273 - val_accuracy: 0.9915\n",
      "Epoch 16/16\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0235 - accuracy: 0.9945 - val_loss: 0.0266 - val_accuracy: 0.9915\n"
     ]
    }
   ],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-4)\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", \n",
    "                     optimizer=optimizer, \n",
    "                     metrics=[\"accuracy\"])\n",
    "\n",
    "history_b_on_A = model_B_on_A.fit(X_train_B, y_train_B, epochs=16, \n",
    "                                  validation_data=(X_val_B, y_val_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0151 - accuracy: 0.9975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.015052733942866325, 0.9975000023841858]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "momentum_optimizer = keras.optimizers.SGD(learning_rate=0.001, \n",
    "                                          momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "nag_optimizer = keras.optimizers.SGD(learning_rate=1e-3, \n",
    "                                     momentum=0.9, \n",
    "                                     nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adagrad_optimizer = keras.optimizers.Adagrad(learning_rate=1e-3, \n",
    "                                             initial_accumulator_value=0.1, \n",
    "                                             epsilon=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop_optimizer = keras.optimizers.RMSprop(learning_rate=1e-3, \n",
    "                                             rho=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_optimizer = keras.optimizers.Adam(learning_rate=0.001, \n",
    "                                       beta_1=0.9, \n",
    "                                       beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1 **(epoch/s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, \n",
    "                                                 patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_of_dataset = 2000\n",
    "s = 20 * length_of_dataset // 32\n",
    "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
    "optimizer = keras.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding Overfitting Through Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "l1_dense_layer = keras.layers.Dense(\n",
    "    units=100, \n",
    "    activation=\"elu\", \n",
    "    kernel_initializer=\"he_normal\", \n",
    "    kernel_regularizer=keras.regularizers.l1(0.01)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_dense_layer = keras.layers.Dense(\n",
    "    units=100, \n",
    "    activation=\"elu\", \n",
    "    kernel_initializer=\"he_normal\", \n",
    "    kernel_regularizer=keras.regularizers.l2(0.01)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_l2_dense_layer = keras.layers.Dense(\n",
    "    units=100, \n",
    "    activation=\"elu\", \n",
    "    kernel_initializer=\"he_normal\", \n",
    "    kernel_regularizer=keras.regularizers.l1_l2(0.01)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(\n",
    "    keras.layers.Dense, \n",
    "    activation=\"elu\", \n",
    "    kernel_initializer=\"he_normal\", \n",
    "    kernel_regularizer=keras.regularizers.l2(0.01)\n",
    ")\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]), \n",
    "    RegularizedDense(300), \n",
    "    RegularizedDense(100), \n",
    "    RegularizedDense(10, activation=\"softmax\", \n",
    "                     kernel_initializer=\"glorot_uniform\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]), \n",
    "    keras.layers.Dropout(rate=0.2), \n",
    "    keras.layers.Dense(300, activation=\"elu\", \n",
    "                       kernel_initializer=\"he_normal\"), \n",
    "    keras.layers.Dropout(rate=0.2), \n",
    "    keras.layers.Dense(100, activation=\"elu\", \n",
    "                       kernel_initializer=\"he_normal\"), \n",
    "    keras.layers.Dropout(rate=0.2), \n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 7s 3ms/step - loss: 0.7499 - accuracy: 0.7347 - val_loss: 0.4495 - val_accuracy: 0.8339\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.5557 - accuracy: 0.8009 - val_loss: 0.4183 - val_accuracy: 0.8449\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.5038 - accuracy: 0.8169 - val_loss: 0.3996 - val_accuracy: 0.8519\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4814 - accuracy: 0.8258 - val_loss: 0.3785 - val_accuracy: 0.8615\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4537 - accuracy: 0.8347 - val_loss: 0.3762 - val_accuracy: 0.8636\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4426 - accuracy: 0.8378 - val_loss: 0.3611 - val_accuracy: 0.8694\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4325 - accuracy: 0.8420 - val_loss: 0.3584 - val_accuracy: 0.8677\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4281 - accuracy: 0.8441 - val_loss: 0.3528 - val_accuracy: 0.8685\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4144 - accuracy: 0.8473 - val_loss: 0.3461 - val_accuracy: 0.8725\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4097 - accuracy: 0.8509 - val_loss: 0.3458 - val_accuracy: 0.8716\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4031 - accuracy: 0.8526 - val_loss: 0.3370 - val_accuracy: 0.8747\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3964 - accuracy: 0.8540 - val_loss: 0.3363 - val_accuracy: 0.8750\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3939 - accuracy: 0.8556 - val_loss: 0.3356 - val_accuracy: 0.8756\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3871 - accuracy: 0.8561 - val_loss: 0.3282 - val_accuracy: 0.8786\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3842 - accuracy: 0.8583 - val_loss: 0.3240 - val_accuracy: 0.8796\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3766 - accuracy: 0.8594 - val_loss: 0.3214 - val_accuracy: 0.8810\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3759 - accuracy: 0.8607 - val_loss: 0.3279 - val_accuracy: 0.8779\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3700 - accuracy: 0.8636 - val_loss: 0.3268 - val_accuracy: 0.8793\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3652 - accuracy: 0.8635 - val_loss: 0.3211 - val_accuracy: 0.8797\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3647 - accuracy: 0.8652 - val_loss: 0.3174 - val_accuracy: 0.8830\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3629 - accuracy: 0.8653 - val_loss: 0.3147 - val_accuracy: 0.8832\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3582 - accuracy: 0.8679 - val_loss: 0.3137 - val_accuracy: 0.8840\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3539 - accuracy: 0.8676 - val_loss: 0.3139 - val_accuracy: 0.8855\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3503 - accuracy: 0.8685 - val_loss: 0.3173 - val_accuracy: 0.8848\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3468 - accuracy: 0.8715 - val_loss: 0.3103 - val_accuracy: 0.8863\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3504 - accuracy: 0.8701 - val_loss: 0.3074 - val_accuracy: 0.8862\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3432 - accuracy: 0.8726 - val_loss: 0.3107 - val_accuracy: 0.8868\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3444 - accuracy: 0.8706 - val_loss: 0.3055 - val_accuracy: 0.8862\n",
      "Epoch 29/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3381 - accuracy: 0.8720 - val_loss: 0.3090 - val_accuracy: 0.8876\n",
      "Epoch 30/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3359 - accuracy: 0.8742 - val_loss: 0.3055 - val_accuracy: 0.8875\n",
      "Epoch 31/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3355 - accuracy: 0.8741 - val_loss: 0.3045 - val_accuracy: 0.8878\n",
      "Epoch 32/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3317 - accuracy: 0.8776 - val_loss: 0.3020 - val_accuracy: 0.8904\n",
      "Epoch 33/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3345 - accuracy: 0.8752 - val_loss: 0.3034 - val_accuracy: 0.8885\n",
      "Epoch 34/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3293 - accuracy: 0.8770 - val_loss: 0.3080 - val_accuracy: 0.8881\n",
      "Epoch 35/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3303 - accuracy: 0.8757 - val_loss: 0.2996 - val_accuracy: 0.8889\n",
      "Epoch 36/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3283 - accuracy: 0.8782 - val_loss: 0.3017 - val_accuracy: 0.8895\n",
      "Epoch 37/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3249 - accuracy: 0.8779 - val_loss: 0.3000 - val_accuracy: 0.8876\n",
      "Epoch 38/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3256 - accuracy: 0.8773 - val_loss: 0.2985 - val_accuracy: 0.8931\n",
      "Epoch 39/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3215 - accuracy: 0.8795 - val_loss: 0.2940 - val_accuracy: 0.8916\n",
      "Epoch 40/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3181 - accuracy: 0.8809 - val_loss: 0.2950 - val_accuracy: 0.8895\n",
      "Epoch 41/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3178 - accuracy: 0.8815 - val_loss: 0.2967 - val_accuracy: 0.8881\n",
      "Epoch 42/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3182 - accuracy: 0.8815 - val_loss: 0.2973 - val_accuracy: 0.8904\n",
      "Epoch 43/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3134 - accuracy: 0.8830 - val_loss: 0.2968 - val_accuracy: 0.8891\n",
      "Epoch 44/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3151 - accuracy: 0.8805 - val_loss: 0.3022 - val_accuracy: 0.8879\n",
      "Epoch 45/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3114 - accuracy: 0.8832 - val_loss: 0.2918 - val_accuracy: 0.8906\n",
      "Epoch 46/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3109 - accuracy: 0.8822 - val_loss: 0.2913 - val_accuracy: 0.8924\n",
      "Epoch 47/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3066 - accuracy: 0.8854 - val_loss: 0.2924 - val_accuracy: 0.8926\n",
      "Epoch 48/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3102 - accuracy: 0.8832 - val_loss: 0.2910 - val_accuracy: 0.8927\n",
      "Epoch 49/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3073 - accuracy: 0.8848 - val_loss: 0.2933 - val_accuracy: 0.8908\n",
      "Epoch 50/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3091 - accuracy: 0.8821 - val_loss: 0.2943 - val_accuracy: 0.8909\n",
      "Epoch 51/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2999 - accuracy: 0.8863 - val_loss: 0.2914 - val_accuracy: 0.8933\n",
      "Epoch 52/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3029 - accuracy: 0.8871 - val_loss: 0.2935 - val_accuracy: 0.8932\n",
      "Epoch 53/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3011 - accuracy: 0.8872 - val_loss: 0.2865 - val_accuracy: 0.8924\n",
      "Epoch 54/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3015 - accuracy: 0.8863 - val_loss: 0.2905 - val_accuracy: 0.8913\n",
      "Epoch 55/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2997 - accuracy: 0.8875 - val_loss: 0.2893 - val_accuracy: 0.8914\n",
      "Epoch 56/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2976 - accuracy: 0.8865 - val_loss: 0.2842 - val_accuracy: 0.8947\n",
      "Epoch 57/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2951 - accuracy: 0.8899 - val_loss: 0.2875 - val_accuracy: 0.8935\n",
      "Epoch 58/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2932 - accuracy: 0.8892 - val_loss: 0.2879 - val_accuracy: 0.8950\n",
      "Epoch 59/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2925 - accuracy: 0.8911 - val_loss: 0.2863 - val_accuracy: 0.8941\n",
      "Epoch 60/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2930 - accuracy: 0.8910 - val_loss: 0.2852 - val_accuracy: 0.8933\n",
      "Epoch 61/100\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.2921 - accuracy: 0.8904 - val_loss: 0.2832 - val_accuracy: 0.8955\n",
      "Epoch 62/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2920 - accuracy: 0.8906 - val_loss: 0.2864 - val_accuracy: 0.8954\n",
      "Epoch 63/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2914 - accuracy: 0.8899 - val_loss: 0.2927 - val_accuracy: 0.8896\n",
      "Epoch 64/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2876 - accuracy: 0.8913 - val_loss: 0.2829 - val_accuracy: 0.8943\n",
      "Epoch 65/100\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.2844 - accuracy: 0.8929 - val_loss: 0.2810 - val_accuracy: 0.8964\n",
      "Epoch 66/100\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.2875 - accuracy: 0.8926 - val_loss: 0.2809 - val_accuracy: 0.8960 loss: 0.2900  - ETA: 0s - loss: 0.2872 - accuracy: \n",
      "Epoch 67/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2859 - accuracy: 0.8930 - val_loss: 0.2852 - val_accuracy: 0.8955\n",
      "Epoch 68/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2843 - accuracy: 0.8917 - val_loss: 0.2893 - val_accuracy: 0.8946\n",
      "Epoch 69/100\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.2849 - accuracy: 0.8932 - val_loss: 0.2848 - val_accuracy: 0.8955\n",
      "Epoch 70/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2833 - accuracy: 0.8923 - val_loss: 0.2835 - val_accuracy: 0.8935\n",
      "Epoch 71/100\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.2813 - accuracy: 0.8951 - val_loss: 0.2853 - val_accuracy: 0.8946\n",
      "Epoch 72/100\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.2800 - accuracy: 0.8931 - val_loss: 0.2824 - val_accuracy: 0.8952\n",
      "Epoch 73/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2797 - accuracy: 0.8943 - val_loss: 0.2798 - val_accuracy: 0.8951\n",
      "Epoch 74/100\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.2794 - accuracy: 0.8950 - val_loss: 0.2840 - val_accuracy: 0.8947\n",
      "Epoch 75/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2750 - accuracy: 0.8957 - val_loss: 0.2795 - val_accuracy: 0.8965\n",
      "Epoch 76/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2797 - accuracy: 0.8944 - val_loss: 0.2831 - val_accuracy: 0.8953\n",
      "Epoch 77/100\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.2778 - accuracy: 0.8955 - val_loss: 0.2814 - val_accuracy: 0.8952\n",
      "Epoch 78/100\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.2780 - accuracy: 0.8944 - val_loss: 0.2796 - val_accuracy: 0.8970\n",
      "Epoch 79/100\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.2755 - accuracy: 0.8960 - val_loss: 0.2810 - val_accuracy: 0.8964\n",
      "Epoch 80/100\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.2743 - accuracy: 0.8965 - val_loss: 0.2789 - val_accuracy: 0.8961\n",
      "Epoch 81/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2697 - accuracy: 0.8971 - val_loss: 0.2813 - val_accuracy: 0.8974\n",
      "Epoch 82/100\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.2721 - accuracy: 0.8978 - val_loss: 0.2800 - val_accuracy: 0.8953\n",
      "Epoch 83/100\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.2743 - accuracy: 0.8951 - val_loss: 0.2793 - val_accuracy: 0.8970\n",
      "Epoch 84/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2707 - accuracy: 0.8975 - val_loss: 0.2758 - val_accuracy: 0.8965\n",
      "Epoch 85/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2686 - accuracy: 0.8983 - val_loss: 0.2772 - val_accuracy: 0.8985\n",
      "Epoch 86/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2707 - accuracy: 0.8979 - val_loss: 0.2787 - val_accuracy: 0.8971\n",
      "Epoch 87/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2663 - accuracy: 0.8983 - val_loss: 0.2771 - val_accuracy: 0.8988\n",
      "Epoch 88/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2664 - accuracy: 0.9002 - val_loss: 0.2824 - val_accuracy: 0.8962\n",
      "Epoch 89/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2634 - accuracy: 0.8997 - val_loss: 0.2832 - val_accuracy: 0.8990\n",
      "Epoch 90/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2653 - accuracy: 0.8997 - val_loss: 0.2804 - val_accuracy: 0.8974\n",
      "Epoch 91/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2670 - accuracy: 0.8988 - val_loss: 0.2782 - val_accuracy: 0.8982\n",
      "Epoch 92/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2651 - accuracy: 0.8997 - val_loss: 0.2813 - val_accuracy: 0.8963\n",
      "Epoch 93/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2668 - accuracy: 0.8995 - val_loss: 0.2821 - val_accuracy: 0.8973\n",
      "Epoch 94/100\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2652 - accuracy: 0.8981 - val_loss: 0.2818 - val_accuracy: 0.8967\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1950ca3e550>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train, X_val = X_train[:50000], X_train[50000:]\n",
    "y_train, y_val = y_train[:50000], y_train[50000:]\n",
    "\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]), \n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(rate=0.2), \n",
    "    keras.layers.Dense(300, activation=\"relu\", \n",
    "                        kernel_initializer=\"he_normal\"), \n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(rate=0.2), \n",
    "    keras.layers.Dense(100, activation=\"relu\", \n",
    "                        kernel_initializer=\"he_normal\"), \n",
    "    keras.layers.Dropout(rate=0.2), \n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "]) \n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, \n",
    "                                                 patience=5)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "              optimizer=\"sgd\", \n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train, epochs=100, \n",
    "          validation_data=(X_val, y_val), \n",
    "          callbacks=[keras.callbacks.EarlyStopping(patience=10, \n",
    "                                                   restore_best_weights=True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"fashion_mnist.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msi1\\AppData\\Local\\Programs\\Python\\Python38\\lib\\contextlib.py:113: UserWarning: `tf.keras.backend.learning_phase_scope` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  return next(self.gen)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "with keras.backend.learning_phase_scope(1):\n",
    "    y_probas = np.stack([model.predict(X_test) for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "        0.998]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(model.predict(X_test[:1]), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]],\n",
       "\n",
       "       [[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.002, 0.   ,\n",
       "         0.998]]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_probas[:, :1], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([X_test[1]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x16b383c4b50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "keras.layers.Dense(100, activation=\"elu\", \n",
    "                   kernel_initializer=\"he_normal\", \n",
    "                   kernel_constraint=keras.constraints.max_norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af68422bec32ee5a6de64dec1179cbe01bf4f7813cec231e0768ec582c2e4b4a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
